[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Esther’s Blog",
    "section": "",
    "text": "Esther’s First Blog\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nEsther Tang\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "In the world of data science, the cleanliness of your dataset can mean the difference clarity and confusion. Today, let’s look at a dataset derived from fiber optic testing. Our mission: to use Power BI to transform this raw data into a clear, analyzable format. Let’s walk through the steps taken to achieve a pristine dataset ready for insightful analysis.\n\n\n\nThe first step is to bring our data into the Power BI environment. We do this by importing the Excel workbook that contains our fiber optic test data. This is done using the Excel.Workbook function, which takes the file path as a parameter and loads the content.\n= Excel.Workbook(File.Contents(\"C:\\Users\\zeony\\Downloads\\Vlad Fast Report\\REFX07 Cable_ Fiber100.xlsm\"), null, true)\n\nThis function navigates to the specified file path, reads the contents of the workbook, and imports the data, setting the stage for the next steps.\n\n\n\nNext, we locate and select the specific sheet named OLTS Report that contains the data we want to analyze. The data is raw at this point, containing all the information from the sheet.\n= Source{[Item=\"OLTS Report\",Kind=\"Sheet\"]}[Data]\n\nThe snippet accesses the OLTS Report sheet within the workbook and extracts all the data from it.\n\n\n\nOur raw data inclused headers and metadata that aren’t necessary for analysis. We remove these by skipping the first 15 rows.\n= Table.Skip(#\"OLTS Report_Sheet\",15)\n\nThe Table.Skip function is used to bypass the first 15 rows of the dataset, which helps in focusing on the data that is actually needed.\n\n\n\nFor our data to be properly organized, we need to promote the first row of our dataset to serve as the headers for our table.\n= Table.PromoteHeaders(#\"Removed Top Rows\", [PromoteAllScalars=true])\n\nThe Table.PromoteHeaders function converts the first row of our adjusted dataset into column headers, giving us a clear, labeled structure to work with.\n\n\n\nTo streamline our dataset, we select only the columns that are necessary for our analysis, removing any extraneous information.\n= Table.SelectColumns(#\"Promoted Headers\",{\"Identifier\", \"Wave-#(lf)length #(lf)(nm)\", \" Loss Average (dB)\", \"Loss Margin#(lf)(dB)\", \"Loss #(lf)A-&gt;B#(lf) (dB)\", \"Loss #(lf)B-&gt;A #(lf)(dB)\", \"ORL#(lf) A#(lf) (dB)\", \"ORL#(lf) B#(lf)(dB)\", \"Length #(lf)#(lf)(ft)\", \"Date/Time\"})\n\nThe Table.SelectColumns function allows us to specify exactly which columns to retain in our dataset, ensuring that we’re working only with data that’s relevant to our analysis goals.\n\n\n\nIn some cases, our dataset may have missing values that can be inferred from the data above. We use the Table.FillDown function to fill these gaps.\n= Table.FillDown(#\"Removed Other Columns\",{\"Identifier\", \"Date/Time\"})\n\nThis function ensures that for any missing entries in the Identifier and Date/Time columns, the value from the previous row is carried down.\n\n\n\nWe filter out rows with identifiers that aren’t necessary for our analysis, such as test identifiers or default values that don’t hold actual test data.\n= Table.SelectRows(#\"Filled Down\", each ([Identifier] &lt;&gt; 1310 and [Identifier] &lt;&gt; 1550 and [Identifier] &lt;&gt; \"1310\" and [Identifier] &lt;&gt; \"1550\" and [Identifier] &lt;&gt; \"Default\" and [Identifier] &lt;&gt; \"Loopback\" and [Identifier] &lt;&gt; \"Pass/Fail Thresholds\" and [Identifier] &lt;&gt; \"Reference\" and [Identifier] &lt;&gt; \"Reference Method\" and [Identifier] &lt;&gt; \"Wavelength#(lf)(nm)\"))\n\nThe Table.SelectRows function is applied with a condition that excludes rows with certain Identifier values, cleaning up our dataset further.\n\n\n\nData types are crucial for accurate analysis. We convert the data in each column to the appropriate data type, such as text, numbers, or dates.\n= Table.TransformColumnTypes(#\"Filtered Rows\",{{\"Identifier\", type text}, {\"Wave-#(lf)length #(lf)(nm)\", Int64.Type}, {\" Loss Average (dB)\", type number}, {\"Loss Margin#(lf)(dB)\", type number}, {\"Loss #(lf)A-&gt;B#(lf) (dB)\", type number}, {\"Loss #(lf)B-&gt;A #(lf)(dB)\", type number}, {\"ORL#(lf) A#(lf) (dB)\", type number}, {\"ORL#(lf) B#(lf)(dB)\", type number}, {\"Length #(lf)#(lf)(ft)\", type number}, {\"Date/Time\", type datetime}})\n\nThis step ensures that each column in our dataset is treated correctly by Power BI during any subsequent operations or analyses.\n\n\n\nSometimes, the last row of data contains summary or footer information that isn’t part of the actual dataset.\n= Table.RemoveLastN(#\"Changed Type\",1)\n\nBy using Table.RemoveLastN, we remove this last row from our dataset to maintain data integrity.\n\n\n\nTo make our analysis more granular, we add a column that uniquely identifies each fiber optic cable.\n= Table.AddColumn(#\"Removed Bottom Rows\", \"CableNumber\", each Number.FromText(Text.AfterDelimiter([Identifier], \"Cable_Fiber\")))\n\n\n\n\nAs we wrap up this segment of our data journey in Power BI, let’s reflect on the significance of what we’ve accomplished so far. We have successfully navigated the intricate process of cleaning and structuring our fiber optic data. Particularly, by transforming the ‘CableNumber’ column to an integer data type, we’ve laid the groundwork for accurate, meaningful analyses:\n= Table.TransformColumnTypes(#\"Added Custom\",{{\"CableNumber\", Int64.Type}})\n\nThis step, while technical, is pivotal. It underscores the essence of data preparation – ensuring that each piece of data is in its most useful form. The ‘CableNumber’ column, now accurately recognized as numerical data, will serve as a key pillar in our subsequent data exploration."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction-to-fiber-optics",
    "href": "posts/post-with-code/index.html#introduction-to-fiber-optics",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "Fiber optics - the invisible threads connecting our digital world - are the arteries of global communication. These hair-thin fibers carry the pulse of the data, voice, and images with astonishing speed and fidelity.\n\n\n\nimage"
  },
  {
    "objectID": "posts/post-with-code/index.html#the-indispensable-nature-of-fiber-optics",
    "href": "posts/post-with-code/index.html#the-indispensable-nature-of-fiber-optics",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "The role of fiber optics extends beyond communication; It is foundational to the data infrastructure that supports our analytical endeavors. Without the clarity and speed they offer, the big data revolution would falter."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-global-web-of-communication",
    "href": "posts/post-with-code/index.html#the-global-web-of-communication",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "Our analysis begins with the acknowledgement of fiber optics’ critical role. These fibers carry more than data; they trasmit opportunities, education, and innovation across oceans with the speed of light."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-challenge-ahead-ensuring-fiber-health",
    "href": "posts/post-with-code/index.html#the-challenge-ahead-ensuring-fiber-health",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "From a data scientist’s perspective, the integrity of these data highways is paramount. A loss average exceeding 3.0dB is not just a number; It signifies potential disruption in the data flow that powers our analyses, models, and predictions."
  },
  {
    "objectID": "posts/post-with-code/index.html#showcasing-power-bi-proficiency-in-data-cleaning",
    "href": "posts/post-with-code/index.html#showcasing-power-bi-proficiency-in-data-cleaning",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "In the language of data science, raw data is the ore from which insights are extracted. Using Power BI, I undertook the following meticulous steps to refine this ore:\n\n\nSource{[Item=\"OLTS Report\",Kind=\"Sheet\"]}[Data] This step imports the OLTS Report sheet data into Power BI. Here, initial inspection reveals two null rows that will need addressing later in the cleaning process.\n\n\n\nTable.TransformColumnTypes(#\"OLTS Report_Sheet\", {...}) Assigns the correct data type to each column of the table. This is crucial as it ensures that calculations and further data manipulations are performed correctly.\n\n\n\nTable.PromoteHeaders(#\"Changed Type\", [PromoteAllScalars=true]) Converts the first row of the dataset into column headers. This step is essential for turning a range of data into a structured table that Power BI can work with effectively.\n\n\n\nTable.Skip(#\"Promoted Headers\",14) Skips the first 14 rows, which are often metadata or formatting that are not relevant to the actual data analysis.\n\n\n\nTable.PromoteHeaders(#\"Removed Top Rows\", [PromoteAllScalars=true]) After removing the top rows, this step re-establishes the first row of the remaining dataset as headers, which may have shifted due to the previous steps.\n\n\n\nTable.FillDown(#\"Promoted Headers1\",{\"Identifier\"}) Ensures that all rows within a particular segment have consistent identifiers, filling down the value from the first row of the segment throughout its rows.\n\n\n\nTable.SelectRows(#\"Filled Down\", each ([Identifier] &lt;&gt; ...)) Removes rows based on the content of the Identifier column, filtering out rows with specific text like “Default” or “Loopback” that are not needed for analysis.\n\n\n\nTable.RemoveColumns(#\"Filtered Rows\",{\"Column2\"}) Removes unnecessary columns from the dataset, in this case, “Column2”, which may contain redundant or irrelevant information.\n\n\n\nTable.SelectColumns(#\"Removed Columns\",{\"Identifier\", \"Wave-#(lf)length #(lf)(nm)\", ...}) Narrows down the dataset to only include columns that are relevant for the analysis.\n\n\n\nTable.FillDown(#\"Removed Other Columns\",{\"Date/Time\"}) Applies the ‘fill down’ operation to the Date/Time column to ensure that all entries within a segment have the correct timestamp.\n\n\n\nTable.TransformColumnTypes(#\"Filled Down1\",{{\"Date/Time\", type datetime}}) Converts the Date/Time column to the datetime data type to enable time-based analysis.\n\n\n\nTable.SelectRows(#\"Changed Type1\", each ([Identifier] &lt;&gt; \"1310\" and [Identifier] &lt;&gt; \"1550\")) Excludes rows with the identifiers “1310” and “1550” if these rows do not contain data relevant to the analysis.\n\n\n\nTable.RemoveLastN(#\"Filtered Rows1\",1) Removes the last row of the dataset, which often contains totals or summary data not needed for individual data point analysis.\n\n\n\nTable.RemoveColumns(#\"Removed Bottom Rows\",{\"Column12\"}) Eliminates the last unnecessary column, further refining the dataset.\n\n\n\nTable.TransformColumnTypes(#\"Removed Columns1\", {...}) Ensures all columns that contain numeric values are set to the correct numeric data type, which is essential for accurate calculations.\n\n\n\nTable.AddColumn(#\"Changed Type2\", \"CableNumber\", each Number.FromText(Text.AfterDelimiter([Identifier], \"Cable_Fiber\"))) Adds a new column that extracts the cable number from the Identifier column, which is a key step for categorizing and segmenting the data.\n\n\n\nTable.TransformColumnTypes(#\"Added Custom\",{{\"CableNumber\", Int64.Type}}) Converts the newly added CableNumber column into an integer data type, which allows for numerical operations to be performed on it."
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "In the world of data science, the cleanliness of your dataset can mean the difference clarity and confusion. Today, let’s look at a dataset derived from fiber optic testing. Our mission: to use Power BI to transform this raw data into a clear, analyzable format. Let’s walk through the steps taken to achieve a pristine dataset ready for insightful analysis."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-1-importing-the-workbook-into-power-bi",
    "href": "posts/post-with-code/index.html#step-1-importing-the-workbook-into-power-bi",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "The first step is to bring our data into the Power BI environment. We do this by importing the Excel workbook that contains our fiber optic test data. This is done using the Excel.Workbook function, which takes the file path as a parameter and loads the content.\n= Excel.Workbook(File.Contents(\"C:\\Users\\zeony\\Downloads\\Vlad Fast Report\\REFX07 Cable_ Fiber100.xlsm\"), null, true)\n\nThis function navigates to the specified file path, reads the contents of the workbook, and imports the data, setting the stage for the next steps."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-2-selecting-the-data",
    "href": "posts/post-with-code/index.html#step-2-selecting-the-data",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "Next, we locate and select the specific sheet named OLTS Report that contains the data we want to analyze. The data is raw at this point, containing all the information from the sheet.\n= Source{[Item=\"OLTS Report\",Kind=\"Sheet\"]}[Data]\n\nThe snippet accesses the OLTS Report sheet within the workbook and extracts all the data from it."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-3-skipping-unneeded-rows",
    "href": "posts/post-with-code/index.html#step-3-skipping-unneeded-rows",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "Our raw data inclused headers and metadata that aren’t necessary for analysis. We remove these by skipping the first 15 rows.\n= Table.Skip(#\"OLTS Report_Sheet\",15)\n\nThe Table.Skip function is used to bypass the first 15 rows of the dataset, which helps in focusing on the data that is actually needed."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-4-establishing-headers",
    "href": "posts/post-with-code/index.html#step-4-establishing-headers",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "For our data to be properly organized, we need to promote the first row of our dataset to serve as the headers for our table.\n= Table.PromoteHeaders(#\"Removed Top Rows\", [PromoteAllScalars=true])\n\nThe Table.PromoteHeaders function converts the first row of our adjusted dataset into column headers, giving us a clear, labeled structure to work with."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-5-selecting-relevant-columns",
    "href": "posts/post-with-code/index.html#step-5-selecting-relevant-columns",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "To streamline our dataset, we select only the columns that are necessary for our analysis, removing any extraneous information.\n= Table.SelectColumns(#\"Promoted Headers\",{\"Identifier\", \"Wave-#(lf)length #(lf)(nm)\", \" Loss Average (dB)\", \"Loss Margin#(lf)(dB)\", \"Loss #(lf)A-&gt;B#(lf) (dB)\", \"Loss #(lf)B-&gt;A #(lf)(dB)\", \"ORL#(lf) A#(lf) (dB)\", \"ORL#(lf) B#(lf)(dB)\", \"Length #(lf)#(lf)(ft)\", \"Date/Time\"})\n\nThe Table.SelectColumns function allows us to specify exactly which columns to retain in our dataset, ensuring that we’re working only with data that’s relevant to our analysis goals."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-6-filling-down-identifiers",
    "href": "posts/post-with-code/index.html#step-6-filling-down-identifiers",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "In some cases, our dataset may have missing values that can be inferred from the data above. We use the Table.FillDown function to fill these gaps.\n= Table.FillDown(#\"Removed Other Columns\",{\"Identifier\", \"Date/Time\"})\n\nThis function ensures that for any missing entries in the Identifier and Date/Time columns, the value from the previous row is carried down."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-7-filtering-out-irrelevant-rows",
    "href": "posts/post-with-code/index.html#step-7-filtering-out-irrelevant-rows",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "We filter out rows with identifiers that aren’t necessary for our analysis, such as test identifiers or default values that don’t hold actual test data.\n= Table.SelectRows(#\"Filled Down\", each ([Identifier] &lt;&gt; 1310 and [Identifier] &lt;&gt; 1550 and [Identifier] &lt;&gt; \"1310\" and [Identifier] &lt;&gt; \"1550\" and [Identifier] &lt;&gt; \"Default\" and [Identifier] &lt;&gt; \"Loopback\" and [Identifier] &lt;&gt; \"Pass/Fail Thresholds\" and [Identifier] &lt;&gt; \"Reference\" and [Identifier] &lt;&gt; \"Reference Method\" and [Identifier] &lt;&gt; \"Wavelength#(lf)(nm)\"))\n\nThe Table.SelectRows function is applied with a condition that excludes rows with certain Identifier values, cleaning up our dataset further."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-8-transforming-column-types",
    "href": "posts/post-with-code/index.html#step-8-transforming-column-types",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "Data types are crucial for accurate analysis. We convert the data in each column to the appropriate data type, such as text, numbers, or dates.\n= Table.TransformColumnTypes(#\"Filtered Rows\",{{\"Identifier\", type text}, {\"Wave-#(lf)length #(lf)(nm)\", Int64.Type}, {\" Loss Average (dB)\", type number}, {\"Loss Margin#(lf)(dB)\", type number}, {\"Loss #(lf)A-&gt;B#(lf) (dB)\", type number}, {\"Loss #(lf)B-&gt;A #(lf)(dB)\", type number}, {\"ORL#(lf) A#(lf) (dB)\", type number}, {\"ORL#(lf) B#(lf)(dB)\", type number}, {\"Length #(lf)#(lf)(ft)\", type number}, {\"Date/Time\", type datetime}})\n\nThis step ensures that each column in our dataset is treated correctly by Power BI during any subsequent operations or analyses."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-9-removing-the-last-row",
    "href": "posts/post-with-code/index.html#step-9-removing-the-last-row",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "Sometimes, the last row of data contains summary or footer information that isn’t part of the actual dataset.\n= Table.RemoveLastN(#\"Changed Type\",1)\n\nBy using Table.RemoveLastN, we remove this last row from our dataset to maintain data integrity."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-10-adding-a-cable-number-column",
    "href": "posts/post-with-code/index.html#step-10-adding-a-cable-number-column",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "To make our analysis more granular, we add a column that uniquely identifies each fiber optic cable.\n= Table.AddColumn(#\"Removed Bottom Rows\", \"CableNumber\", each Number.FromText(Text.AfterDelimiter([Identifier], \"Cable_Fiber\")))"
  },
  {
    "objectID": "posts/post-with-code/index.html#step-11-transforming-data-type",
    "href": "posts/post-with-code/index.html#step-11-transforming-data-type",
    "title": "Esther’s First Blog",
    "section": "",
    "text": "As we wrap up this segment of our data journey in Power BI, let’s reflect on the significance of what we’ve accomplished so far. We have successfully navigated the intricate process of cleaning and structuring our fiber optic data. Particularly, by transforming the ‘CableNumber’ column to an integer data type, we’ve laid the groundwork for accurate, meaningful analyses:\n= Table.TransformColumnTypes(#\"Added Custom\",{{\"CableNumber\", Int64.Type}})\n\nThis step, while technical, is pivotal. It underscores the essence of data preparation – ensuring that each piece of data is in its most useful form. The ‘CableNumber’ column, now accurately recognized as numerical data, will serve as a key pillar in our subsequent data exploration."
  }
]